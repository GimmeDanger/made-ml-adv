# Advanced ML at MADE, 2020

Это курс об основах байесовского вывода и вероятностных моделях в машинном обучении, представленный в [Академии больших данных Mail.Ru](https://data.mail.ru/) в 2020 году. 

# Lectures
1. Введение. История AI. Что такое машинное обучение, какие задачи оно решает. Байесовский подход в машинном обучении, теорема Байеса и её интерпретация. Байесовский анализ испытаний Бернулли: [запись](https://www.youtube.com/watch?v=H3LVXu7tSmg)
2. Введение в байесовский вывод: полный анализ бросаний монетки, теорема Байеса в жизни, hot hand fallacy. Линейная регрессия: метод наименьших квадратов: [запись](https://www.youtube.com/watch?v=HF8MlOPvB5Y)
3. Линейная регрессия: вероятностный смысл, априорные распределения, оверфиттинг и регуляризация, предсказания, эквивалентное ядро: [запись](https://www.youtube.com/watch?v=mCczRVUuQoQ)
4. Несколько важных сюжетов: байесовский выбор моделей, проклятие размерности, разложение bias-variance-noise: [запись](https://www.youtube.com/watch?v=PtVXHjkozbs)
5. Задачи классификации: геометрический смысл, порождающие модели и LDA/QDA, логистическая регрессия: [запись](https://www.youtube.com/watch?v=yi_pstsjvAs)
6. Байесовский вывод для гауссиана. Кластеризация и EM-алгоритм для кластеризации, примеры: [запись](https://www.youtube.com/watch?v=RPdNfdaxRwg)
7. EM-алгоритм в общем виде. Обоснование EM для кластеризации. Простые примеры EM: считаем аллели генов. Проспективные и ретроспективные исследования. EM-алгоритм для presence-only data: [запись](https://www.youtube.com/watch?v=sdoRjVTrHjg)
8. Модели Брэдли-Терри и MM-алгоритмы. Скрытые марковские модели: три задачи, их решения, алгоритм Баума-Велха, скрытые марковские модели для распознавания речи: [запись](https://www.youtube.com/watch?v=D4NytAKriWs&list=PLIZk0YRTQGyN5TcJFkGwI7kymZ0I27uBa&index=9)
9. Графические вероятностные модели. Алгоритм передачи сообщений: [запись](https://www.youtube.com/watch?v=85HZL6jOxkU&list=PLIZk0YRTQGyN5TcJFkGwI7kymZ0I27uBa&index=10)
10. Сэмплирование для приближённого вывода: постановка задачи, сэмплирование с отклонением, сэмплирование с весами значимости. MCMC-методы: алгоритм Метрополиса-Гастинга и сэмплирование по Гиббсу: [запись](https://www.youtube.com/watch?v=V8oc3FQ1U7I&list=PLIZk0YRTQGyN5TcJFkGwI7kymZ0I27uBa&index=11)
11. Развёрнутый пример: SIR-модели в эпидемиологии. Как сходятся вместе Monte Carlo EM, HMM со стохастическим алгоритмом Витерби и оценки presence-only data: [запись](https://www.youtube.com/watch?v=OiCn-CANkq0&list=PLIZk0YRTQGyN5TcJFkGwI7kymZ0I27uBa&index=12)
12. Вариационные приближения: основные идеи, примеры с гауссианами. Вариационный вывод в смеси гауссиан: [запись](https://www.youtube.com/watch?v=YIyUFmstrFM&list=PLIZk0YRTQGyN5TcJFkGwI7kymZ0I27uBa&index=13)
13. Развёрнутый пример: от наивного байесовского классификатора к тематическому моделированию: [запись](https://www.youtube.com/watch?v=K13JNoYbZEc&list=PLIZk0YRTQGyN5TcJFkGwI7kymZ0I27uBa&index=14)

Весь плейлист: [запись](https://www.youtube.com/playlist?list=PLIZk0YRTQGyN5TcJFkGwI7kymZ0I27uBa)

# Домашние задания
1. Теорема Байеса и линейная регрессия: что делать с коронавирусом?
2. Рейтинг-система для спортивного «Что? Где? Когда?».
3. MCMC-сэмплирование и «пляшущие человечки».

# Selected references
1. Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, Information Science and Statistics series, 2006.
2. Kevin Murphy. Machine Learning: A Probabilistic Perspective, MIT Press, 2012.
3. David J. C. MacKay. Information Theory, Inference, and Learning Algorithms. Cambridge University Press, 2003.
4. С.И. Николенко, А.А. Кадурин, Е.О. Архангельская. Глубокое обучение. Питер, 2018.
5. Fintzi et al. Efficient data augmentation for fitting stochastic epidemic models to prevalence data, arXiv:1606.07995, 2016.
